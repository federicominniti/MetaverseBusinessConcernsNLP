{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "%matplotlib inline"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": true,
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "trusted": false,
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "data = pd.read_csv(\"../data/tweets_final.csv\")"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "a498ee778ab763e0801b8f9cf14e1d4d01f38846",
    "_cell_guid": "3e44d856-a323-45ac-b7cc-80d77385060f",
    "trusted": false,
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "data = data.iloc[0:100,:]"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "punc = ['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}',\"%\"]\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(punc)\n",
    "desc = data.Text.values\n",
    "vectorizer = TfidfVectorizer(stop_words = stop_words)\n",
    "X = vectorizer.fit_transform(desc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "word_features = vectorizer.get_feature_names()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z\\']+')\n",
    "\n",
    "def tokenize(text):\n",
    "    return [stemmer.stem(word) for word in tokenizer.tokenize(text.lower())]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "971\n",
      "[\"'u\", 'abl', 'access', 'acent', 'action', 'actualizacion', 'adttnf', 'advantag', 'age', 'ago', 'ai', 'airdrop', 'alert', 'alienworld', 'alison', 'altcoin', 'altern', 'alvin', 'ama', 'amaz', 'amazid', 'ambit', 'amidst', 'amp', 'android', 'ani', 'anim', 'anti', 'ap', 'ape', 'apesinvad', 'ar', 'arena', 'arriba', 'art', 'articl', 'asset', 'assistir', 'aswqdx', 'automat', 'auwh', 'av', 'avail', 'avatar', 'avax', 'avoid', 'axieinfin', 'ay', 'azka', 'b']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "vectorizer2 = TfidfVectorizer(stop_words = stop_words, tokenizer = tokenize)\n",
    "X2 = vectorizer2.fit_transform(desc)\n",
    "word_features2 = vectorizer2.get_feature_names()\n",
    "print(len(word_features2))\n",
    "print(word_features2[:50])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "vectorizer3 = TfidfVectorizer(stop_words = stop_words, tokenizer = tokenize, max_features = 1000)\n",
    "X3 = vectorizer3.fit_transform(desc)\n",
    "words = vectorizer3.get_feature_names()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "d = pd.DataFrame(X3.toarray())\n",
    "d.to_csv(\"../data/data_to_cluster.csv\",header=words,index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : t, https, metavers, p, e, nftcommun, kucoin, mr, like, dk, bof, pvagscx, vr, tanukivers, play, u, need, nft, black, build, ethereumnft, o, theapeinvad, projectseedgam, vsx\n",
      "1 : web, t, project, https, nftgiveaway, metavers, nft, nfts, token, nftproject, d, verasaw, vr, communiti, luxuryrealest, l, great, got, qlbqhc, live, list, s, metac, time, featur\n",
      "2 : gold, otraeconomia, othereconomi, ethereum, cryptocurr, bitcoin, nftcommun, n, nfts, nft, usd, es, m, portug, euro, el, yin, dxi, pfi, lo, s, metavers, vjyrtbohmk, yypohh, oro\n",
      "3 : wishforbul, snw, nft, b, metavers, supernftwar, https, t, gama, qus, evou, bc, upland, beog, jzktn, buidl, elpkb, realitychain, metadogworld, race, vn, galuka, moon, hoangyen, juliapham\n",
      "4 : otherde, whaleanalytica, bought, world, bayc, ape, othersid, mayc, nftworld, nftworldsnft, https, t, ggrxygqhwu, zxmgysbojb, metavers, cvkyluf, tsew, owya, ap, jtr, v, nft, c, b, gad\n",
      "5 : defi, bsc, java, python, blockhain, nodej, btc, drop, solananft, metavers, cryptocurr, shibainu, hold, buy, pictur, look, trust, shil, okay, crypto, https, t, nft, cryptomin, gcd\n",
      "6 : mintabl, store, nftnyc, https, check, nft, t, nftartist, nftcommmun, nftart, nfts, flow, metavers, crypto, nftcollector, gasless, opensea, father, nftdrop, age, thefacelessmani, nyc, riseup, spatialxr, mira\n",
      "7 : metavers, meta, icebergi, news, titl, btc, bitcoin, mtrt, epic, look, launch, crypto, use, street, anim, technolog, trend, amp, fur, new, current, world, digit, https, t\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters = 8, n_init = 20)\n",
    "kmeans.fit(X3)\n",
    "# Finally, we look at 8 the clusters generated by k-means.\n",
    "common_words = kmeans.cluster_centers_.argsort()[:,-1:-26:-1]\n",
    "for num, centroid in enumerate(common_words):\n",
    "    print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "version": "3.6.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}